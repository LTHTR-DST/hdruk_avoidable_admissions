{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Avoidable Admissions","text":"<p>This Python package is being developed as part of a federated multi-site collaboration led by the School of Health and Related Research at Sheffield University and coordinated by HDRUK.</p> <p>The study documentation is maintained at https://mattstammers.github.io/hdruk_avoidable_admissions_collaboration_docs.</p> <p>The <code>avoidable_admissions</code> Python package is documented here along with examples.</p>"},{"location":"#installation","title":"Installation","text":"<p>Please see the README.md file for more information on environment setup for contributing to development.</p> <p>The following describes installation of the package within an existing environment. A separate virtual environment is recommended.</p> <p>The package maybe installed directly from GitHub using one of the following commands:</p> <p>To install only the package:</p> <pre><code>pip install \"avoidable_admissions @ git+https://github.com/LTHTR-DST/hdruk_avoidable_admissions.git@&lt;release-name&gt;\"\n</code></pre> <p>To install with optional dependencies for exploratory data analysis:</p> <pre><code>pip install \"avoidable_admissions[eda] @ git+https://github.com/LTHTR-DST/hdruk_avoidable_admissions.git@&lt;release-name&gt;\"\n</code></pre> <p>To install with optional dependecies for contributing to development and documentation:</p> <pre><code>pip install \"avoidable_admissions[dev] @ git+https://github.com/LTHTR-DST/hdruk_avoidable_admissions.git@&lt;release-name&gt;\"\n</code></pre> <p>Replace <code>&lt;release-name&gt;</code> with the latest release version e.g. <code>v0.3.1</code>. List of releases can be found here - https://github.com/LTHTR-DST/hdruk_avoidable_admissions/releases.</p> <p>Omit <code>@&lt;release-name&gt;</code> to install the latest code in the repo.</p> <p>See https://lthtr-dst.github.io/hdruk_avoidable_admissions/admitted_care_pipeline_example for a complete example.</p>"},{"location":"admitted_care_pipeline_example/","title":"Avoidable Admissions Analysis Pipeline Example","text":""},{"location":"admitted_care_pipeline_example/#admitted-care-data-set","title":"Admitted Care Data Set","text":"<pre><code>Site: Lancashire Teaching Hospitals NHS Trust\nCreated: 2023-02-04\n\nLead site: SCHARR Institute, Sheffield University\n</code></pre>"},{"location":"admitted_care_pipeline_example/#references","title":"References","text":"<ul> <li>Statistical Analysis Plan - https://docs.google.com/document/d/1mpRKxNDbkTPwDhg7S_AaOXwbFQvHjXQRL7Qa31zn7DY/edit</li> <li>Data Processing - https://docs.google.com/document/d/1vysTKmvELK-5Rr7Dib3zDp8mCe_lUVr2e5EES23ShbQ/edit</li> <li>Analysis Tables - https://docs.google.com/document/d/10PuNTnEG5zTkWOVaMGlfOInleXJ0KA-_5hSH4upwhi4/edit</li> <li>LTH GitHub Repo - https://github.com/LTHTR-DST/hdruk_avoidable_admissions</li> <li>Pipeline Docs - https://lthtr-dst.github.io/hdruk_avoidable_admissions/</li> <li>Collaboration Docs - https://mattstammers.github.io/hdruk_avoidable_admissions_collaboration_docs/</li> </ul> <pre><code>%load_ext autoreload\n%autoreload 2\n\nfrom datetime import datetime\n\nnow = datetime.today()\nprint(\"Starting pipeline execution at\", now)\n</code></pre> <pre><code>Starting pipeline execution at 2023-02-05 11:51:15.151223\n</code></pre> <pre><code>import os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nload_dotenv(\"../.env\")\n\ndir_data = Path(os.getenv(\"DIR_DATA\"))\ndir_data\n</code></pre> <pre><code>WindowsPath('T:/Business Intelligence/Data Science/Work/hdruk_avoidable_adms')\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom IPython.display import HTML\n\nfrom avoidable_admissions.data.validate import (\n    AdmittedCareEpisodeSchema,\n    AdmittedCareFeatureSchema,\n    get_schema_properties,\n    validate_admitted_care_data,\n    validate_admitted_care_features,\n    validate_dataframe,\n)\nfrom avoidable_admissions.features import feature_maps\nfrom avoidable_admissions.features.build_features import build_admitted_care_features\n</code></pre>"},{"location":"admitted_care_pipeline_example/#load-data","title":"Load Data","text":"<pre><code># df_admcare = pd.read_csv(dir_data.joinpathpath('raw', 'admitted_care.csv')\ndf_admcare = pd.read_csv(\"../data/raw/admitted_care.csv\")\n</code></pre> <pre><code># Create a copy of the data to fix DQ issues to avoid reloading data from source everytime\ndfa = df_admcare.copy()\n\n\"Raw dataframe contains %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Raw dataframe contains 47483 rows and 63 columns'\n</code></pre>"},{"location":"admitted_care_pipeline_example/#pipeline-overview","title":"Pipeline Overview","text":"<p>See https://lthtr-dst.github.io/hdruk_avoidable_admissions/pipeline/ for more details.</p>"},{"location":"admitted_care_pipeline_example/#first-validation","title":"First Validation","text":"<p>This is done using the raw data typically extracted from SQL databases maintained by Business Intelligence.</p> <p>Receiving the data in a format that matches the Sheffield specification as closely as possible will make subsequent steps easier.</p> <p>Always, always, have friends in BI. Who is your Quin?</p>"},{"location":"admitted_care_pipeline_example/#admitted-care-episode-validation-rules","title":"Admitted Care Episode Validation Rules","text":"<p>Use <code>get_schema_properties(Schema)</code> for an overview of what the expectations of the first validation schema are.</p> <p>Help improve the validation rules and documentation, eg. title, description, etc. by contributing to the GitHub repo or raising an issue.</p> <pre><code>schema_1 = (\n    get_schema_properties(AdmittedCareEpisodeSchema)\n    .sort_values(\"name\")\n    .set_index(\"name\")\n)\nschema_1\n</code></pre> dtype nullable unique coerce required checks regex title name admiage int64 False False False True [&lt;Check greater_than_or_equal_to: greater_than_or_equal_to(18)&gt;, &lt;Check less_than_or_equal_to: less_than_or_equal_to(130)&gt;] False None admidate date False False False True [&lt;Check greater_than_or_equal_to: greater_than_or_equal_to(2021-10-01)&gt;, &lt;Check less_than_or_equal_to: less_than_or_equal_to(2022-09-30)&gt;] False None admimeth str True False False True [&lt;Check isin: isin({'31', '32', '28', '12', '24', '2A', '98', '82', '23', '81', '25', '13', '83', '2C', '21', '2D', '22', '2B', '11', '99'})&gt;] False None admisorc str True False False True [&lt;Check isin: isin({'53', '54', '66', '55', '65', '39', '37', '19', '98', '87', '79', '51', '88', '86', '52', '40', '56', '85', '29', '49', '99'})&gt;] False None admitime str True False True True [&lt;Check str_matches: str_matches(re.compile('2[0-3]|[01]?[0-9]:[0-5][0-9]'))&gt;] False None diag_01 str True False False True [] False None diag_[0-9]{2} str True False False True [] True None disdest str True False False True [&lt;Check isin: isin({'53', '54', '66', '48', '30', '65', '49', '39', '37', '19', '98', '87', '79', '84', '51', '88', '52', '85', '50', '29', '38', '99'})&gt;] False None dismeth str True False False True [&lt;Check isin: isin({'2', '8', '4', '1', '5', '9', '3'})&gt;] False None epiorder int64 True False False True [&lt;Check greater_than_or_equal_to: greater_than_or_equal_to(0)&gt;] False None ethnos str False False False True [&lt;Check isin: isin({'C', 'H', 'M', 'N', 'J', 'R', 'B', 'L', 'A', 'G', 'K', 'S', 'P', 'E', 'Z', 'F', '99', 'D'})&gt;] False None gender str False False False True [&lt;Check isin: isin({'2', 'X', '1', '9', '0'})&gt;] False None length_of_stay float64 True False False True [&lt;Check greater_than_or_equal_to: greater_than_or_equal_to(0)&gt;] False None opdate_01 datetime64[ns] True False False True [] False None opdate_[0-9]{2} datetime64[ns] True False False True [] True None opertn_01 str True False False True [] False None opertn_[0-9]{2} str True False False True [] True None patient_id int64 False False False True [] False None procodet str False False False True [] False None sitetret str False False False True [] False None townsend_score_decile int64 False False False True [&lt;Check greater_than_or_equal_to: greater_than_or_equal_to(0)&gt;, &lt;Check less_than_or_equal_to: less_than_or_equal_to(10)&gt;] False None visit_id int64 False True False True [] False None"},{"location":"admitted_care_pipeline_example/#validate-using-admittedcareepisodeschema","title":"Validate using AdmittedCareEpisodeSchema","text":"<p>The first time we try to validate the raw data, there are several errors for multiple reasons.</p> <p>Spending a few minutes reading the schema validation error messages as well as reading the documentation for Pandera will save days for this and future projects.</p> <p><code>validate_dataframe(dfa, AdmittedCareEpisodeSchema)</code> is equivalent to <code>validate_admitted_care_data(dfa)</code></p> <pre><code>good, bad = validate_dataframe(dfa, AdmittedCareEpisodeSchema)\n\nprint(\"Good dataframe has %d rows\" % good.shape[0])\nprint(\"Bad dataframe has %d rows\" % bad.shape[0])\n</code></pre> <pre><code>Schema AdmittedCareEpisodeSchema: A total of 18 schema errors were found.\n\nError Counts\n------------\n- column_not_in_dataframe: 1\n- schema_component_check: 17\n\nSchema Error Summary\n--------------------\n                                                                                                                                                                                                                                     failure_cases  n_failes\nschema_context  column                check\nDataFrameSchema &lt;NA&gt;                  column_in_dataframe                                                                                                                                                                               [visit_id]         1\nColumn          admiage               greater_than_or_equal_to(18)                                                                                                                                   [0,3,12,14,4,15,2,6,11,5,1,16,17,10,7,8,13,9]        18\n                admidate              greater_than_                                                                                                               [TypeError(\"'&gt;=' not supported between instances of 'str' and 'datetime.date'\")]         1\n                                      less_than_or_equal_to(2022-09-30)                                                                                           [TypeError(\"'&lt;=' not supported between instances of 'str' and 'datetime.date'\")]         1\n                admimeth              isin({'31','32','28','12','24','2A','98','82','23','81','25','13','83','2C','21','2D','22','2B','11','99'})                                                                                             [27]         1\n                admisorc              dtype('str')                                                                                                                                                                                         [int64]         1\n                                      isin({'53','54','66','55','65','39','37','19','98','87','79','51','88','86','52','40','56','85','29','49','99'})                                                 [51,19,66,52,29,56,99,88,40,49,33,87,79,53]        14\n                disdest               dtype('str')                                                                                                                                                                                         [int64]         1\n                                      isin({'53','54','66','48','30','65','49','39','37','19','98','87','79','84','51','88','52','85','50','29','38','99'})                       [19,99,79,88,29,51,52,85,84,54,38,50,37,66,65,53,87,49,48,98,30]        21\n                dismeth               dtype('str')                                                                                                                                                                                         [int64]         1\n                                      isin({'2','8','4','1','5','9','3'})                                                                                                                                                              [1,4,2,8,9]         5\n                gender                dtype('str')                                                                                                                                                                                         [int64]         1\n                                      isin({'2','X','1','9','0'})                                                                                                                                                                          [2,1,9]         3\n                opdate_01             dtype('datetime64[ns]')                                                                                                                                                                             [object]         1\n                townsend_score_decile dtype('int64')                                                                                                                                                                                     [float64]         1\n                                      not_nullable                                                                                                                                                                                           [nan]         1\n\nUsage Tip\n---------\n\nDirectly inspect all errors by catching the exception:\n\n```\ntry:\n    schema.validate(dataframe, lazy=True)\nexcept SchemaErrors as err:\n    err.failure_cases  # dataframe of schema errors\n    err.data  # invalid dataframe\n```\n\nNo data will pass validation due to column error. See output above.\nGood dataframe has 0 rows\nBad dataframe has 246321 rows\n</code></pre>"},{"location":"admitted_care_pipeline_example/#iterative-dq-fixes","title":"Iterative DQ Fixes","text":"<p>Fix one column or one error at a time and rerun <code>validate_dataframe(dfa, AdmittedCareEpisodeSchema)</code> in the previous cell to check if it has worked.</p> <p>If you have made an error in data transformation, rerun <code>dfa = df_admcare.copy()</code> to start again.</p> <p>Error Behaviour: In case of errors other than SchemaErrors, <code>validate_dataframe</code> will print out the error message and return an empty <code>good</code> data frame and a <code>bad</code> dataframe containing all rows in input dataframe. Column errors, i.e., missing, misspelt or unexpected additional columns will also result in identical behaviour.</p> <p>The <code>bad</code> dataframe may contain more rows than the input dataframe, as each error generates a new row. For instance, if one row in the input dataframe resulted in 5 errors in different columns, this will generate 5 rows in the <code>bad</code> dataframe, each with details on the specific errors available in additional columns.</p> <p>TODO: vc to fix. If you get a <code>ufunc 'bitwise_or' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''</code> error, this is usually caused by a dtype mismatch between <code>datetime</code> in the input data when the validation code expects a <code>date</code> dtype.</p> <p>Feature Maps: These set out what values are allowed in various columns to pass first validation, how these will be transformed during feature engineering and what values are allowed in the new columns to pass second validation. https://github.com/LTHTR-DST/hdruk_avoidable_admissions/blob/dev/avoidable_admissions/features/feature_maps.py</p> <p>_Help improve this process by raising a GitHub issue if validation fails when it shouldn't or you run into unexpected errors.</p> <pre><code>\"Starting with %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Starting with 47483 rows and 63 columns'\n</code></pre> <pre><code>dfa[\"visit_id\"] = dfa.reset_index(drop=True).index\n</code></pre> <pre><code>dfa.patient_id = dfa.patient_id.astype(np.int64)\n</code></pre> <pre><code>dfa = dfa.loc[dfa.admiage &gt;= 18].copy()\n\"Dataframe has %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Dataframe has 39168 rows and 64 columns'\n</code></pre> <pre><code>cols_opdate = dfa.filter(regex=\"opdate\").columns\ndfa[cols_opdate] = dfa[cols_opdate].apply(pd.to_datetime, dayfirst=True)\n</code></pre> <pre><code>dfa.admidate = pd.to_datetime(dfa.admidate, dayfirst=True).dt.date\n</code></pre> <pre><code># dfa.admimeth = dfa.admimeth.astype(str)\ndfa = dfa[dfa.admimeth != \"27\"]  # Careful\n\"Dataframe has %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Dataframe has 39167 rows and 64 columns'\n</code></pre> <pre><code>dfa.admisorc = dfa.admisorc.astype(str)\ndfa = dfa[dfa.admisorc.isin(feature_maps.admisorc)]\ndfa = dfa[dfa.admimeth != \"27\"]  # Careful\n\"Dataframe has %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Dataframe has 39166 rows and 64 columns'\n</code></pre> <pre><code>dfa.gender = dfa.gender.astype(str)\n</code></pre> <pre><code>dfa.disdest = dfa.disdest.astype(str)\ndfa = dfa[dfa.disdest.isin(feature_maps.disdest)]\n\"Dataframe has %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Dataframe has 39166 rows and 64 columns'\n</code></pre> <pre><code>dfa.dismeth = dfa.dismeth.astype(str)\ndfa = dfa[dfa.dismeth.isin(feature_maps.dismeth)]\n\"Dataframe has %d rows and %d columns\" % dfa.shape\n</code></pre> <pre><code>'Dataframe has 39166 rows and 64 columns'\n</code></pre> <pre><code># Set missing IMD values to 0 to pass validation. Address how this is dealth with at the analytics stage.\n# This is to get around the quirks of what nan means in the Pandas (and Pandera) world.\ndfa.townsend_score_decile = dfa.townsend_score_decile.fillna(0).astype(np.int64)\n</code></pre> <pre><code># If everything above worked, this should not throw any errors.\ngood, bad = validate_admitted_care_data(dfa)\n\nprint(\"Good dataframe has %d rows\" % good.shape[0])\nprint(\"Bad dataframe has %d rows\" % bad.shape[0])\n</code></pre> <pre><code>Good dataframe has 39166 rows\nBad dataframe has 0 rows\n</code></pre>"},{"location":"admitted_care_pipeline_example/#feature-engineering","title":"Feature engineering","text":"<p>See documentation here: https://lthtr-dst.github.io/hdruk_avoidable_admissions/features/</p> <p>This has been written to Sheffield's specifications and will change if the specification changes.</p> <p>_Help improve this process by raising a GitHub issue or contributing code.</p> <pre><code>dff = build_admitted_care_features(good.copy())\n</code></pre>"},{"location":"admitted_care_pipeline_example/#second-validation","title":"Second validation","text":"<p>Second validation and addressing DQ is identical to the first validation process.</p>"},{"location":"admitted_care_pipeline_example/#admitted-care-feature-validation-rules","title":"Admitted Care Feature Validation Rules","text":"<p>The features dataframe should contain all columns from the initial dataframe and the new generated features.</p> <p>Once again have a look at <code>feature_maps.py</code> for more details.</p> <p>Let's look at only the new features and the rules that apply to them.</p> <pre><code>schema_2 = (\n    get_schema_properties(AdmittedCareFeatureSchema)\n    .sort_values(\"name\")\n    .set_index(\"name\")\n)\nschema_2[~schema_2.index.isin(schema_1.index)]\n</code></pre> dtype nullable unique coerce required checks regex title description name admiage_cat str False False False True [&lt;Check isin: isin({'65 - 69', '25 - 29', '75 ... False None None admidayofweek None True False False True [&lt;Check isin: isin({'Sunday', 'Wednesday', 'Mo... False None None admisorc_cat None True False False True [&lt;Check isin: isin({'Medical care', 'Residence... False None None diag_01_acsc None False False False True [&lt;Check isin: isin({'COPD', 'Pneumothorax ', '... False None None diag_seasonal_cat None True False False True [&lt;Check isin: isin({'Respiratory infection', '... False None None disdest_cat None True False False True [&lt;Check isin: isin({'Medical care', 'Died', 'C... False None None dismeth_cat None True False False True [&lt;Check isin: isin({'Died', 'Discharged', 'Unk... False None None ethnos_cat str True False False True [&lt;Check isin: isin({'Other Ethnic Groups', 'As... False None None gender_cat str False False False True [&lt;Check isin: isin({'Female', 'Indeterminate',... False None None length_of_stay_cat None True False False True [&lt;Check isin: isin({'&gt;=2 days', '&lt;2 days'})&gt;] False None None opertn_count int64 False False False True [&lt;Check greater_than_or_equal_to: greater_than... False None None townsend_score_quintile int64 True False False True [&lt;Check in_range: in_range(0, 5)&gt;] False None None"},{"location":"admitted_care_pipeline_example/#fix-minor-dq-issues-and-validate-using-admittedcarefeatureschema","title":"Fix minor DQ issues and validate using AdmittedCareFeatureSchema","text":"<pre><code># TODO: vc to fix. Easiest will be to validate this as datetime rather than date\ndff.admidate = dff.admidate.dt.date\n</code></pre> <pre><code># TODO: vc to fix. build_features makes this a categorical variable. Needs to be fixed either in build_features or in the validation schema\ndff.admiage_cat = dff.admiage_cat.astype(str)\n</code></pre> <pre><code>good_f, bad_f = validate_dataframe(dff, AdmittedCareFeatureSchema)\nprint(\"Good dataframe has %d rows\" % good_f.shape[0])\nprint(\"Bad dataframe has %d rows\" % bad_f.shape[0])\n</code></pre> <pre><code>Good dataframe has 39166 rows\nBad dataframe has 0 rows\n</code></pre>"},{"location":"admitted_care_pipeline_example/#analysis","title":"Analysis","text":"<p>Refer to https://docs.google.com/document/d/10PuNTnEG5zTkWOVaMGlfOInleXJ0KA-_5hSH4upwhi4/edit for details.</p> <pre><code>df = good_f.copy()\n</code></pre> <p>This section describes the analysis populations or the analysis of the APC data. These populations will be referred to in the analysis descriptions that follow.</p> <p>All Acute Admissions</p> <p>This analysis population includes all acute admissions.</p> <p>Filter: (admimeth in 21, 22, 23, 24, 25, 2A, 2B, 2C, 2D, 28) &amp; (epiorder = 1)</p> <p>These filters should have been applied to the original data extract.</p> <p>All Acute Admissions for ACSCs</p> <p>This analysis population includes all acute admissions where the primary diagnosis from the first episode is an ACSC.</p> <p>Filter: (admimeth in 21, 22, 23, 24, 25, 2A, 2B, 2C, 2D, 28) &amp; (epiorder = 1) &amp; (diag_01 = ACSC_Code_APC)</p> <p>All Acute Admissions for Non-ACSCs</p> <p>This analysis population includes all acute admissions where the primary diagnosis from the first episode is not an ACSC.</p> <p>Filter: (admimeth in 21, 22, 23, 24, 25, 2A, 2B, 2C, 2D, 28) &amp; (epiorder = 1) &amp; (diag_01 != ACSC_Code_APC)</p> <pre><code># All acute admissions - this should be redundant if done at extraction\ndf = df[\n    (df.admimeth.isin({\"21\", \"22\", \"23\", \"24\", \"25\", \"2A\", \"2B\", \"2C\", \"2D\", \"28\"}))\n    &amp; (df.epiorder == 1)\n]\n\n# Acute admissions by ACSC status\ndf[\"is_acsc\"] = df.diag_01_acsc.where(df.diag_01_acsc == \"-\", \"ACSC\").replace(\n    \"-\", \"Non-ACSC\"\n)\n</code></pre>"},{"location":"admitted_care_pipeline_example/#age","title":"Age","text":"<pre><code>pd.concat(\n    [df.admiage.describe().rename(\"All\"), df.groupby(\"is_acsc\").admiage.describe().T],\n    axis=1,\n).T\n</code></pre> count mean std min 25% 50% 75% max All 3#.## 5#.## 2#.## 1#.## 3#.## 6#.## 7#.## 1#.## ACSC 1#.## 6#.## 1#.## 1#.## 5#.## 6#.## 8#.## 1#.## Non-ACSC 2#.## 5#.## 2#.## 1#.## 3#.## 5#.## 7#.## 1#.## <pre><code>categorical_features = {\n    \"admiage_cat\": \"Age Bands\",\n    \"gender_cat\": \"Gender\",\n    \"ethnos_cat\": \"Ethnicity\",\n    \"townsend_score_quintile\": \"Townsend Score Quintile\",\n    \"admisorc_cat\": \"Admission Source\",\n    \"admidayofweek\": \"Admission Day of Week\",\n    \"diag_seasonal_cat\": \"Seasonal Diagnosis\",\n    \"length_of_stay_cat\": \"Length of Stay\",\n    \"disdest_cat\": \"Discharge Destination\",\n    \"dismeth_cat\": \"Discharge Method\",\n}\n</code></pre> <pre><code>def make_crosstab(colname, tablename):\n    x = pd.crosstab(df[k], df.is_acsc, margins=True, dropna=False, margins_name=\"Total\")\n\n    y = (\n        pd.crosstab(\n            df[k],\n            df.is_acsc,\n            normalize=\"index\",\n            dropna=False,\n            margins_name=\"Total\",\n        )\n        .mul(100)\n        .round(2)\n        .rename(columns={\"ACSC\": \"ACSC %\", \"Non-ACSC\": \"Non-ACSC %\"})\n    )\n\n    z = pd.concat([x, y], axis=1).sort_index(axis=1).fillna(\"-\")\n\n    z.index = pd.MultiIndex.from_tuples([(v, i) for i in z.index])\n\n    return z\n</code></pre> <pre><code># HTML Output\nout = \"\"\ndf_results = []\nfor k, v in categorical_features.items():\n    z = make_crosstab(k, v)\n    df_results.append(z)\n    out += f\"\"\"\n        ### {v}\n{z.to_html()}\n\n        \"\"\"\ndf_results = pd.concat(df_results)\nHTML(out)\n</code></pre>"},{"location":"admitted_care_pipeline_example/#age-bands","title":"Age Bands","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Age Bands 18-19 1### 1#.## 7### 8#.## 9### 20 - 24 4### 2#.## 1### 7#.## 2### 25 - 29 4### 1#.## 2### 8#.## 2### 30 - 34 5### 2#.## 1### 7#.## 2### 35 - 39 5### 2#.## 1### 7#.## 2### 40 - 44 6### 3#.## 1### 6#.## 1### 45 - 49 7### 3#.## 1### 6#.## 1### 50 - 54 9### 4#.## 1### 5#.## 2### 55 - 59 1### 4#.## 1### 5#.## 2### 60 - 64 1### 4#.## 1### 5#.## 2### 65 - 69 1### 4#.## 1### 5#.## 2### 70 - 74 1### 4#.## 1### 5#.## 3### 75 - 79 1### 4#.## 1### 5#.## 3### 80 - 84 1### 4#.## 1### 5#.## 3### &gt;85 2### 4#.## 2### 5#.## 4### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#gender","title":"Gender","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Gender Female 8### 3#.## 1### 6#.## 2### Indeterminate 4### 3#.## 9### 6#.## 1### Male 7### 4#.## 9### 5#.## 1### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#ethnicity","title":"Ethnicity","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Ethnicity Asian or Asian British 7### 3#.## 1### 6#.## 2### Black or Black British 1### 3#.## 1### 6#.## 3### Mixed 9### 3#.## 2### 6#.## 3### Not known 1### 0#.## 1### 1#.## 1### Not stated 5### 2#.## 1### 7#.## 2### Other Ethnic Groups 1### 3#.## 2### 6#.## 4### White 1### 4#.## 2### 5#.## 3### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#townsend-score-quintile","title":"Townsend Score Quintile","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Townsend Score Quintile 0 1### 2#.## 2### 7#.## 3### 1 3### 3#.## 5### 6#.## 9### 2 3### 3#.## 4### 6#.## 7### 3 2### 3#.## 3### 6#.## 5### 4 3### 3#.## 5### 6#.## 8### 5 3### 4#.## 4### 5#.## 7### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#admission-source","title":"Admission Source","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Admission Source Care Home 2### 1#.## 1### 8#.## 1### Medical care 8### 3#.## 1### 6#.## 2### Penal 5### 5#.## 5### 5#.## 1### Residence 1### 3#.## 2### 6#.## 3### Unknown 1### 1#.## 5### 8#.## 6### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#admission-day-of-week","title":"Admission Day of Week","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Admission Day of Week Friday 2### 4#.## 3### 5#.## 6### Monday 2### 3#.## 3### 6#.## 5### Saturday 1### 3#.## 2### 6#.## 4### Sunday 1### 3#.## 2### 6#.## 3### Thursday 2### 4#.## 3### 5#.## 6### Tuesday 2### 3#.## 3### 6#.## 6### Wednesday 2### 4#.## 3### 5#.## 6### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#seasonal-diagnosis","title":"Seasonal Diagnosis","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Seasonal Diagnosis - 1### 3#.## 2### 6#.## 3### Chronic disease exacerbation 1### 9#.## 9### 7#.## 1### Respiratory infection 1### 8#.## 2### 1#.## 2### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#length-of-stay","title":"Length of Stay","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Length of Stay &lt;2 days 7### 3#.## 1### 6#.## 1### &gt;=2 days 8### 4#.## 1### 5#.## 1### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#discharge-destination","title":"Discharge Destination","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Discharge Destination Care Home 2### 3#.## 5### 6#.## 7### Died 1### 3#.## 3### 6#.## 4### Medical care 1### 2#.## 5### 7#.## 6### Penal 3### 1#.## 2### 8#.## 2### Residence 1### 4#.## 1### 5#.## 3### Unknown 1### 3#.## 2### 6#.## 4### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#discharge-method","title":"Discharge Method","text":"is_acsc ACSC ACSC % Non-ACSC Non-ACSC % Total Discharge Method Died 5### 3#.## 1### 6#.## 1### Discharged 1### 3#.## 2### 6#.## 3### Not Applicable 1### 0#.## 1### 1#.## 1### Unknown 1### 0#.## 1### 1#.## 1### Total 1### - 2### - 3###"},{"location":"admitted_care_pipeline_example/#export-output-for-sheffield","title":"Export Output for Sheffield","text":"<pre><code>timestamp = now.strftime(\"%Y_%m_%dT%H%M%S\")\ntimestamp\n</code></pre> <pre><code>'2023_02_05T115115'\n</code></pre>"},{"location":"admitted_care_pipeline_example/#export-as-csv","title":"Export as CSV","text":"<pre><code># save df_results to a CSV to send to Sheffield for programmatic meta-analysis\n# df_results.to_csv('path/to/results_timestamp.csv')\n</code></pre>"},{"location":"admitted_care_pipeline_example/#export-as-html","title":"Export as HTML","text":"<pre><code>output_head = f\"\"\"\n&lt;h1&gt; LTH Admitted Care Analysis Tables Draft&lt;/h1&gt;\n\n&lt;pre&gt;\nprepared: {timestamp}\nemail:    datascience@lthtr.nhs.uk\ngithub:   &lt;a href='https://github.com/LTHTR-DST/'&gt;https://github.com/LTHTR-DST/&lt;/a&gt;\n&lt;/pre&gt;\n\"\"\"\n\nfp = f\"../reports/lth_admitted_care_analysis_tables_draft_{timestamp}.html\"\nwith open(fp, \"w\") as f:\n    x = df_results.to_html()\n    x = output_head + x\n    f.write(x)\n</code></pre> <pre><code>print(\n    \"Finished pipeline execution in\",\n    round((datetime.today() - now).total_seconds()),\n    \"seconds at\",\n    datetime.today(),\n)\n</code></pre> <pre><code>Finished pipeline execution in 9 seconds at 2023-02-05 11:51:24.288155\n</code></pre> <p>The above outputs can easily be shared and if all sites used the same code, Sheffield should be able to generate the 'meta-analysis' more easily.</p> <p>END OF NOTEBOOK</p>"},{"location":"features/","title":"Feature Engineering","text":"<p>Feature engineering is the process of generating new variables from one or more existing variables.</p> <p>The Data Processing document defined by the lead team provides excellent and explicit documentation on what new features are expected. Refer to these documents for more details.</p> <ul> <li>HDRUK Data Processing V1 Google Docs</li> </ul> <p>The functions described below generate these features automatically in preparation for the second validation step and further analysis.</p> <p>Ensure that data has undergone preprocessing and has passed the first validation step as described in the analysis pipeline before using these functions.</p>"},{"location":"features/#error-codes","title":"Error codes","text":"<p>A pragmatic approach has been used in dealing with missing data, unmapped codes and codes not in refsets. Please read section on missing values in the Data Validation chapter as well.</p> <p>During feature engineering, especially in the Emergency Care dataset that has several columns with SNOMED codes, the following rules are applied to assign the appropriate categories.</p> Source Data Mapping Refset Category Who fixes Yes Yes Yes Assign to <code>Category</code> Yes No Yes <code>ERROR:Unmapped - In Refset</code> Lead site to advise Yes Yes No <code>ERROR:Not In Refset|{Category}</code> Lead site to fix No x x <code>ERROR:Missing Data</code> Local site if feasible Yes No No <code>ERROR:Unmapped - Not In Refset</code> Local site to fix <p>Please see the source code for <code>feature_maps.py</code> and raise a GitHub issue for any questions or bugs.</p> <p>Read the source code for generating admitted care features and emergency care features on GitHub.</p>"},{"location":"features/#avoidable_admissions.features.build_features.build_admitted_care_features","title":"<code>build_admitted_care_features(df)</code>","text":"<p>Generate features described in the Admitted Care Data Specification</p> <p>See Analysis Pipeline for more information</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Pandas DataFrame</code> <p>Dataframe that has passed the first validation step</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with additional feature columns</p>"},{"location":"features/#avoidable_admissions.features.build_features.build_admitted_care_features--feature-engineering-example","title":"Feature Engineering Example:","text":"<pre><code>import pandas as pd\nfrom avoidable_admissions.data.validate import (\n    validate_dataframe,\n    AdmittedCareEpisodeSchema\n)\nfrom avoidable_admissions.features.build_features import (\n    build_admitted_care_features\n)\n\n\n# Load raw data typically extracted using SQL from source database\ndf = pd.read_csv('../data/raw/admitted_care)\n\n# First validation step using Episode Schema\n# Review, fix DQ issues and repeat this step until all data passes validation\ngood, bad = validate_dataframe(df, AdmittedCareEpisodeSchema)\n\n# Feature engineering using the _good_ dataframe\ndf_features = build_admitted_care_features(good)\n\n# Second validation step and continue...\n</code></pre> <p>See Analysis Pipeline for more information.</p> Source code in <code>avoidable_admissions/features/build_features.py</code> <pre><code>def build_admitted_care_features(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Generate features described in the Admitted Care Data Specification\n\n    See [Analysis Pipeline][data-analysis-pipeline] for more information\n\n    Args:\n        df (Pandas DataFrame): Dataframe that has passed the first validation step\n\n    Returns:\n        pd.DataFrame: Dataframe with additional feature columns\n\n\n    ## Feature Engineering Example:\n\n    ``` python\n    import pandas as pd\n    from avoidable_admissions.data.validate import (\n        validate_dataframe,\n        AdmittedCareEpisodeSchema\n    )\n    from avoidable_admissions.features.build_features import (\n        build_admitted_care_features\n    )\n\n\n    # Load raw data typically extracted using SQL from source database\n    df = pd.read_csv('../data/raw/admitted_care)\n\n    # First validation step using Episode Schema\n    # Review, fix DQ issues and repeat this step until all data passes validation\n    good, bad = validate_dataframe(df, AdmittedCareEpisodeSchema)\n\n    # Feature engineering using the _good_ dataframe\n    df_features = build_admitted_care_features(good)\n\n    # Second validation step and continue...\n    ```\n\n    See [Analysis Pipeline][data-analysis-pipeline] for more information.\n    \"\"\"\n\n    df = admitted_care_features.build_all(df)\n\n    return df\n</code></pre>"},{"location":"features/#avoidable_admissions.features.build_features.build_emergency_care_features","title":"<code>build_emergency_care_features(df)</code>","text":"Source code in <code>avoidable_admissions/features/build_features.py</code> <pre><code>def build_emergency_care_features(df: pd.DataFrame) -&gt; pd.DataFrame:\n\n    df = emergency_care_features.build_all(df)\n\n    return df\n</code></pre>"},{"location":"pipeline/","title":"Data Analysis Pipeline","text":"<p>Data harmonisation is a vital step in constructing a reproducible, reusable data analytics pipeline.</p> <p>Please refer to the detailed data specification and analysis plan developed by the lead team at Sheffield University.</p> <p>These documentation as well as detailed, step-by-step information on setting up a Python environment and getting started on this project are available at https://mattstammers.github.io/hdruk_avoidable_admissions_collaboration_docs/.</p> <p>The Avoidable Admissions project requires the preparation and analysis of 2 distinct datasets - for admitted care and emergency care. The steps are identical for both and are shown in the flow chart below.</p> <p>Click on the flowchart elements for more information as it applies to the Admitted Care Dataset. Similar functions are available for the Emergency Care Dataset.</p> <pre><code>flowchart TB\n    subgraph Admitted_Care_Pipeline\n        direction TB\n        subgraph Preprocessing\n            A(Extract) --&gt; B(Validate)\n            B --&gt; C{Errors?}\n            C --&gt;|Yes| D(Fix Errors)\n            D --&gt; B\n        end\n        subgraph Feature_Engineering\n            C --&gt;|No| E(Generate Features)\n            E --&gt; F(Validate)\n            F --&gt; G{Errors?}\n            G --&gt;|Yes| H(Fix Errors)\n            H --&gt; F\n        end\n        subgraph Analysis\n            G --&gt;|No| I(Analysis)\n        end\n    end\n    style A stroke:#526cfe,stroke-width:4px\n    style E stroke:#526cfe,stroke-width:4px\n    style I stroke:#526cfe,stroke-width:4px\n\n    style B stroke:#26b079, stroke-width:4px\n    style F stroke:#26b079, stroke-width:4px\n\n    style D stroke:#ff7872\n    style H stroke:#ff7872\n\n\n    click B \"/hdruk_avoidable_admissions/validation/#avoidable_admissions.data.validate.validate_admitted_care_data\"\n    click F \"/hdruk_avoidable_admissions/validation/#avoidable_admissions.data.validate.validate_admitted_care_features\"\n\n    click C \"/hdruk_avoidable_admissions/validation/#avoidable_admissions.data.validate.validate_dataframe--validation-example\"\n    click G \"/hdruk_avoidable_admissions/validation/#avoidable_admissions.data.validate.validate_dataframe--validation-example\"\n\n\n    click E \"/hdruk_avoidable_admissions/features/#avoidable_admissions.features.build_features.build_admitted_care_features\"\n\n    click D \"/hdruk_avoidable_admissions/validation/#fixing-errors\"\n</code></pre>"},{"location":"pipeline/#pipeline-example","title":"Pipeline Example","text":"<p>This is an example using the Admitted Care Dataset. The same principles apply for the Emergency Care Dataset.</p> <pre><code>import pandas as pd\nfrom avoidable_admissions.data.validate import (\n    validate_dataframe,\n    AdmittedCareEpisodeSchema,\n    AdmittedCareFeatureSchema\n)\nfrom avoidable_admissions.features.build_features import (\n    build_admitted_care_features\n)\n\n\n# Load raw data typically extracted using SQL from source database\ndf = pd.read_csv(\"../data/raw/admitted_care.csv\")\n\n# First validation step using Episode Schema\n# Review, fix DQ issues and repeat this step until all data passes validation\ngood, bad = validate_dataframe(df, AdmittedCareEpisodeSchema)\n\n# Feature engineering using the _good_ dataframe\ndf_features = build_admitted_care_features(good)\n\n# Second validation step using Feature Schema\n# Review and fix DQ issues.\n# This may require returning to the first validation step or even extraction.\ngood_f, bad_f = validate_dataframe(df_features, AdmittedCareFeatureSchema)\n\n# Use the good_f dataframe for analysis as required by lead site\n</code></pre> <p>Please see Pipeline Example for a more detailed Jupyter notebook.</p>"},{"location":"validation/","title":"Data Validation","text":"<p>There are multiple schema defined for validation of different datasets at different pipeline stages.</p>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe","title":"<code>validate_dataframe(df, schema, **kwargs)</code>","text":"<p>Validates data against a specified schema.</p> <p>The data should have been prepared as per the specification set by the lead site. Use the output of this function to iteratively identify and address data quality issues.</p> <p>The following schema are defined:</p> <p>Admitted Care Data:</p> <ul> <li><code>AdmittedCareEpisodeSchema</code></li> <li><code>AdmittedCareFeatureSchema</code></li> </ul> <p>Emergency Care Data:</p> <ul> <li><code>EmergencyCareEpisodeSchema</code></li> <li><code>EmergencyCareFeatureSchema</code></li> </ul> <p>See source code for validation rules.</p> <p>Returns a good dataframe containing rows that passed validation and a bad dataframe with rows that failed validation. The bad dataframe has additional columns that provide information on failure cause(s). If there is a column error (misspelt, missing or additional), all rows will be returned in bad dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to be validated</p> required <code>schema</code> <code>DataFrameSchema</code> <p>Pandera schema to validate against</p> required <code>kwargs</code> <p>The following keyword arguments are currently supported</p> <code>{}</code> <code>start_date</code> <code>datetime</code> <p>Study start date (inclusive)</p> required <code>end_date</code> <code>datetime</code> <p>Study end date (excluded)</p> required <code>ignore_cols</code> <code>list</code> <p>Columns to ignore during validation checks.</p> required <code>update_cols</code> <code>dict[str</code> <p>dict]): Dictionary of column:properties to update schema.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Good and Bad dataframes. See example below.</p>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--validation-example","title":"Validation example","text":"<pre><code>from avoidable_admissions.data.validate import (\n    validate_dataframe,\n    AdmittedCareEpisodeSchema\n)\n\n\ndf = pd.read_csv('path/to/data.csv')\ngood, bad = validate_dataframe(df, AdmittedCareEpisodeSchema)\n</code></pre> <p>If <code>df</code> had rows that fail validation, the function will print an output similar to below.</p> <pre><code>Schema AdmittedCareEpisodeSchema: A total of 1 schema errors were found.\n\nError Counts\n------------\n- schema_component_check: 1\n\nSchema Error Summary\n--------------------\n                                                    failure_cases  n_failure_cases\nschema_context column  check\nColumn         admiage greater_than_or_equal_to(18)        [17.0]                1\n</code></pre> <p>This message indicates that there was a validation error in the <code>admiage</code> column which expects values &gt;=18.</p> <p>Fix data quality iteratively to ensure there are no errors.</p> <p>If you find a bug in the validation code, and correct data fails validation, please raise a GitHub issue.</p>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--customising-validation","title":"Customising validation","text":""},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--customise-study-dates","title":"Customise study dates","text":"<p>As a default, the study dates specified in the initial protocol will be used (<code>admidate&gt;=datetime(2021,11,1) and admidate&lt;datetime(2022,11,1)</code>). However, these can be altered by providing these as keyword arguments.</p> <p>The following rule is applied to <code>admidate</code> in the Acute Admissions dataset and to <code>edarrivaldatetime</code> in the emergency care dataset.</p> <p><code>admidate&gt;=start_date</code> and <code>admidate&lt;end_date</code></p> <p>The <code>&lt;end_date</code> allows <code>31-10-2022 23:59:00</code> to pass validation when <code>end_date</code> is set to <code>datetime(2022,11,1)</code>.</p>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--ignore-selected-columns","title":"Ignore selected columns","text":"<p>Passing a list of column names to<code>ignore_cols</code> as a keyword argument will apply the following properties, effectively turning off validation.</p> <pre><code>{\n    'dtype': None,\n    'checks': [],\n    'nullable': False,\n    'unique': False,\n    'coerce': False,\n    'required': True\n}\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--update-validation-rules","title":"Update validation rules","text":"<p>Passing a dictionary of {column_name: property_dict} allows fine-grained control. For example, to update remove checks on <code>edchiefcomplaint</code> but preserve other validation rules, pass the following to <code>update_cols</code>.</p>"},{"location":"validation/#avoidable_admissions.data.validate.validate_dataframe--custom-validation-example","title":"Custom validation example","text":"<p>The example below applies the following custom rules:</p> <ul> <li>Change study start and end dates</li> <li>Ignore validation on <code>eddiag_NN</code> columns.     This requires both _01 and _[0-9]{2}$ regex suffixes to be set.     Note the $ at the end of regex.</li> <li>Don't perform checks on <code>edchiefcomplaint</code> but retain other rules e.g dtype</li> <li>Dont' check data type for <code>accommodationstatus</code> but retain other rules</li> </ul> <pre><code>good, bad = validate_dataframe(\n    df,\n    schema,\n    start_date=datetime(2021, 10, 1),\n    end_date=datetime(2022, 11, 1),\n    ignore_cols=[\"eddiag_01\", \"eddiag_[0-9]{2}$\"],\n    update_cols={\n        \"edchiefcomplaint\": {\"checks\": []},\n        \"accommodationstatus\": {\"dtype\": None},\n    }\n)\n</code></pre> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def validate_dataframe(\n    df: pd.DataFrame, schema: pa.DataFrameSchema, **kwargs\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"Validates data against a specified schema.\n\n    The data should have been prepared as per the specification set by the lead site.\n    Use the output of this function to iteratively identify and address data quality issues.\n\n    The following schema are defined:\n\n    Admitted Care Data:\n\n    - `AdmittedCareEpisodeSchema`\n    - `AdmittedCareFeatureSchema`\n\n    Emergency Care Data:\n\n    - `EmergencyCareEpisodeSchema`\n    - `EmergencyCareFeatureSchema`\n\n    See __[source code](https://github.com/LTHTR-DST/hdruk_avoidable_admissions/blob/dev/avoidable_admissions/data/validate.py)__\n    for validation rules.\n\n    Returns a _good_ dataframe containing rows that passed validation and\n    a _bad_ dataframe with rows that failed validation.\n    The _bad_ dataframe has additional columns that provide information on failure cause(s).\n    If there is a column error (misspelt, missing or additional), all rows will be returned in _bad_ dataframe.\n\n    Args:\n        df (pandas.DataFrame): Dataframe to be validated\n        schema (pa.DataFrameSchema): Pandera schema to validate against\n        kwargs: The following keyword arguments are currently supported\n        start_date (datetime): Study start date (inclusive)\n        end_date (datetime): Study end date (excluded)\n        ignore_cols (list): Columns to ignore during validation checks.\n        update_cols (dict[str:dict]): Dictionary of column:properties to update schema.\n\n    Returns:\n        _Good_ and _Bad_ dataframes. See example below.\n\n    ## Validation example\n\n    ``` python\n    from avoidable_admissions.data.validate import (\n        validate_dataframe,\n        AdmittedCareEpisodeSchema\n    )\n\n\n    df = pd.read_csv('path/to/data.csv')\n    good, bad = validate_dataframe(df, AdmittedCareEpisodeSchema)\n    ```\n\n    If `df` had rows that fail validation, the function will print an output similar to below.\n\n        Schema AdmittedCareEpisodeSchema: A total of 1 schema errors were found.\n\n        Error Counts\n        ------------\n        - schema_component_check: 1\n\n        Schema Error Summary\n        --------------------\n                                                            failure_cases  n_failure_cases\n        schema_context column  check\n        Column         admiage greater_than_or_equal_to(18)        [17.0]                1\n\n    This message indicates that there was a validation error in the `admiage` column which expects values &gt;=18.\n\n    Fix data quality iteratively to ensure there are no errors.\n\n    If you find a bug in the validation code, and correct data fails validation,\n    please raise a [GitHub issue](https://github.com/LTHTR-DST/hdruk_avoidable_admissions/issues).\n\n    ## Customising validation\n\n    ### Customise study dates\n\n    As a default, the study dates specified in the initial protocol will be used\n    (`admidate&gt;=datetime(2021,11,1) and admidate&lt;datetime(2022,11,1)`).\n    However, these can be altered by providing these as keyword arguments.\n\n    The following rule is applied to `admidate` in the Acute Admissions dataset\n    and to `edarrivaldatetime` in the emergency care dataset.\n\n    `admidate&gt;=start_date` and `admidate&lt;end_date`\n\n    The `&lt;end_date` allows `31-10-2022 23:59:00` to pass validation when\n    `end_date` is set to `datetime(2022,11,1)`.\n\n    ### Ignore selected columns\n\n    Passing a list of column names to`ignore_cols` as a keyword argument will\n    apply the following properties, effectively turning off validation.\n\n    ```python\n    {\n        'dtype': None,\n        'checks': [],\n        'nullable': False,\n        'unique': False,\n        'coerce': False,\n        'required': True\n    }\n    ```\n\n    ### Update validation rules\n\n    Passing a dictionary of {column_name: property_dict} allows fine-grained control.\n    For example, to update remove checks on `edchiefcomplaint` but preserve\n    other validation rules, pass the following to `update_cols`.\n\n    ### Custom validation example\n\n    The example below applies the following custom rules:\n\n    - Change study start and end dates\n    - Ignore validation on `eddiag_NN` columns.\n        This requires both *_01* and *_[0-9]{2}$* regex suffixes to be set.\n        Note the _$_ at the end of regex.\n    - Don't perform checks on `edchiefcomplaint` but retain other rules e.g dtype\n    - Dont' check data type for `accommodationstatus` but retain other rules\n\n\n    ```python\n    good, bad = validate_dataframe(\n        df,\n        schema,\n        start_date=datetime(2021, 10, 1),\n        end_date=datetime(2022, 11, 1),\n        ignore_cols=[\"eddiag_01\", \"eddiag_[0-9]{2}$\"],\n        update_cols={\n            \"edchiefcomplaint\": {\"checks\": []},\n            \"accommodationstatus\": {\"dtype\": None},\n        }\n    )\n    ```\n\n\n    \"\"\"\n\n    df_errors = pd.DataFrame()\n\n    # todo: document this behaviour to warn user that index will be dropped.\n    # alternatively find a way to set a unique key for each row - important for merging errors\n    df = df.copy().reset_index(drop=True)\n\n    start_date = kwargs.get(\"start_date\", datetime(2021, 11, 1))\n    end_date = kwargs.get(\"end_date\", datetime(2022, 11, 1))\n\n    date_checks = [\n        pa.Check.ge(start_date),\n        pa.Check.lt(end_date),\n    ]\n\n    if schema.name.startswith(\"AdmittedCare\"):\n        cohort_date_col = \"admidate\"\n    elif schema.name.startswith(\"EmergencyCare\"):\n        cohort_date_col = \"edarrivaldatetime\"\n\n    updated_column_props = {}\n\n    updated_column_props[cohort_date_col] = {\"checks\": date_checks}\n\n    # New feature - allow user to ignore checks on some columns\n    ignore_cols = kwargs.get(\"ignore_cols\", [])\n\n    update_cols = kwargs.get(\"update_cols\", {})\n\n    blank_props = {\n        \"dtype\": None,\n        \"checks\": [],\n        \"nullable\": False,\n        \"unique\": False,\n        \"coerce\": False,\n        \"required\": True,\n    }\n\n    for col in ignore_cols:\n        updated_column_props[col] = blank_props\n\n    updated_column_props.update(update_cols)\n\n    # If a column in ignore_cols is not present in schema, this will raise\n    # a SchemaInitError with name of column causing the error.\n    schema = schema.update_columns(updated_column_props)\n\n    try:\n        # Capture all errors\n        # https://pandera.readthedocs.io/en/stable/lazy_validation.html\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            schema.validate(df, lazy=True)\n    except pa.errors.SchemaErrors as ex:\n\n        print(ex.args[0])\n\n        df_errors = ex.failure_cases.copy()\n\n        # First get the rows that are causing errors\n        df_errors[\"index\"] = df_errors[\"index\"].fillna(df.index.max() + 1)\n        df_errors = df.merge(df_errors, how=\"right\", left_index=True, right_on=\"index\")\n        df_errors[\"index\"] = df_errors[\"index\"].replace(df.index.max() + 1, None)\n\n        # Column name mismatches will have an 'index' of NaN which causes merge to fail\n        # If a column name is not present, then all rows should be returned as errors\n\n        if df_errors[\"index\"].hasnans:  # this\n            # there is a column error. drop all rows from the 'good' dataframe\n            df = df.iloc[0:0]\n\n            print(\"No data will pass validation due to column error. See output above.\")\n\n        else:\n            df = df[~df.index.isin(df_errors[\"index\"])]\n    except Exception as ex:\n        # This is to catch all other errors.\n        print(ex.args[0])\n        print(\n            \"No data will pass validation due to undefined error.\"\n            \"See output above and please raise an issue on GitHub.\"\n        )\n\n        df = df.iloc[0:0]\n        df_errors = df.copy()\n\n    finally:\n        return df, df_errors\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.validate_admitted_care_data","title":"<code>validate_admitted_care_data(df, **kwargs)</code>","text":"<p>Convenience wrapper for <code>validate_dataframe(df, AdmittedCareEpisodeSchema)</code></p> <p>See avoidable_admissions.data.validate.validate_dataframe for usage.</p> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def validate_admitted_care_data(\n    df: pd.DataFrame, **kwargs\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"Convenience wrapper for `validate_dataframe(df, AdmittedCareEpisodeSchema)`\n\n    See [avoidable_admissions.data.validate.validate_dataframe][] for usage.\n    \"\"\"\n\n    return validate_dataframe(df, AdmittedCareEpisodeSchema, **kwargs)\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.validate_admitted_care_features","title":"<code>validate_admitted_care_features(df, **kwargs)</code>","text":"<p>Convenience wrapper for <code>validate_dataframe(df, AdmittedCareFeatureSchema)</code></p> <p>See avoidable_admissions.data.validate.validate_dataframe for usage.</p> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def validate_admitted_care_features(\n    df: pd.DataFrame, **kwargs\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"Convenience wrapper for `validate_dataframe(df, AdmittedCareFeatureSchema)`\n\n    See [avoidable_admissions.data.validate.validate_dataframe][] for usage.\n    \"\"\"\n    return validate_dataframe(df, AdmittedCareFeatureSchema, **kwargs)\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.validate_emergency_care_data","title":"<code>validate_emergency_care_data(df, **kwargs)</code>","text":"<p>Convenience wrapper for <code>validate_dataframe(df, EmergencyCareEpisodeSchema)</code></p> <p>See avoidable_admissions.data.validate.validate_dataframe for usage.</p> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def validate_emergency_care_data(\n    df: pd.DataFrame, **kwargs\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"Convenience wrapper for `validate_dataframe(df, EmergencyCareEpisodeSchema)`\n\n    See [avoidable_admissions.data.validate.validate_dataframe][] for usage.\n    \"\"\"\n    return validate_dataframe(df, EmergencyCareEpisodeSchema, **kwargs)\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.validate_emergency_care_features","title":"<code>validate_emergency_care_features(df, **kwargs)</code>","text":"<p>Convenience wrapper for <code>validate_dataframe(df, EmergencyCareFeatureSchema)</code></p> <p>See avoidable_admissions.data.validate.validate_dataframe for usage.</p> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def validate_emergency_care_features(\n    df: pd.DataFrame, **kwargs\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"Convenience wrapper for `validate_dataframe(df, EmergencyCareFeatureSchema)`\n\n    See [avoidable_admissions.data.validate.validate_dataframe][] for usage.\n    \"\"\"\n    return validate_dataframe(df, EmergencyCareFeatureSchema, **kwargs)\n</code></pre>"},{"location":"validation/#avoidable_admissions.data.validate.get_schema_properties","title":"<code>get_schema_properties(schema)</code>","text":"<p>Get detailed information about a validation schema including checks, dtypes, nullability and more.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Pandera DataFrameSchema</code> <p>One of <code>AdmittedCareEpisodeSchema</code>, <code>AdmittedCareFeatureSchema</code>,</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe containing schema properties.</p> Source code in <code>avoidable_admissions/data/validate.py</code> <pre><code>def get_schema_properties(schema: pa.DataFrameSchema) -&gt; pd.DataFrame:\n\"\"\"Get detailed information about a validation schema including checks, dtypes, nullability and more.\n\n    Args:\n        schema (Pandera DataFrameSchema): One of `AdmittedCareEpisodeSchema`, `AdmittedCareFeatureSchema`,\n        `EmergencyCareEpisodeSchema`, `EmergencyCareFeatureSchema`\n\n    Returns:\n        pd.DataFrame: Dataframe containing schema properties.\n    \"\"\"\n\n    df = pd.DataFrame([x.properties for x in schema.columns.values()])\n\n    columns = [\n        \"name\",\n        \"dtype\",\n        \"nullable\",\n        \"unique\",\n        \"coerce\",\n        \"required\",\n        \"checks\",\n        \"regex\",\n        \"title\",\n        \"description\",\n    ]\n\n    return df[columns]\n</code></pre>"},{"location":"validation/#fixing-errors","title":"Fixing Errors","text":"<p>It is likely that data validation will fail on a subset of the data the first few times. Fixing errors will be an iterative process and the following are some examples.</p> <p>Please see https://mattstammers.github.io/hdruk_avoidable_admissions_collaboration_docs/ for more examples.</p> <p>Errors in validation after feature generation may be caused by extraneous codes that are not specified in the data specification.</p>"},{"location":"validation/#examples","title":"Examples","text":"<pre><code># Convert to date\n\ndf['admidate'] = pd.to_datetime(df['admidate'], yearfirst=True)\ndf['admidate'] = df['admidate'].dt.date\n\n\n# Fill missing SNOMED codes with 0.\n# Else valiation will fail as nan is treated as float.\ndf['accommodationstatus'] = df['accommodationstatus'].fillna(0)\n</code></pre>"},{"location":"validation/#missing-values","title":"Missing Values","text":"<p>To be finalised after further discussion and testing.</p> <p>There is an entire chapter in Pandas documentation on missing values which is an important read for any data scientist.</p> <p>For the purposes of this project, several pragmatic choices have been made regarding how missing values are treated.</p> <ol> <li>Where a definition exists for how missing values should be coded, for instance in the NHS data model, use this.</li> <li>For SNOMED codes, which are always integers, use 0 (zero) to replace all missing values. This avoids validation errors caused by <code>NaN</code> values that are treated as <code>float</code> dtype by Pandas.</li> <li>For strings, use <code>\"-\"</code> (without the quotes) for missing values.</li> <li>During feature engineering, custom error values are assigned to codes that are missing from either the refsets or mapping.</li> </ol>"}]}