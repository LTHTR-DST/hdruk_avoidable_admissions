{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deaa25d-3336-45a2-9472-b25924461154",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b5fe3-01df-4dae-9089-876695bf9616",
   "metadata": {},
   "source": [
    "# HDRUK Acute Admissions Data Extraction\n",
    "\n",
    "This is an example using sythetic data generated by LTHTR.\n",
    "\n",
    "It is still under development and there are bound to be breaking changes and bugs. \n",
    "\n",
    "Please see commit history, ensure you have the latest clone of the repo, have updated your conda environment using the environment.yaml file.\n",
    "\n",
    "Please raise an issue if you find a bug or have a question.\n",
    "\n",
    "The pseduodata used in this script uses artificial data generated using MIMIC-III as per the method described by https://github.com/nhsx/SynthVAE but there are many other methods available described here: https://nhsx.github.io/AnalyticsUnit/synthetic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddf5d6-c2b9-4992-bdfa-80e56af0c130",
   "metadata": {},
   "source": [
    "## Admitted Care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b5f11-7f5a-4929-beed-f6cff5812459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import avoidable_admissions as aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deeda0e-dae3-4634-937b-f08575267702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data. Typically df is used as a moniker for a dataframe. The dytype argument can be droped if you want (it coerces all the data to strings)\n",
    "\n",
    "df = pd.read_csv(\"synthetic_data/hdruk_acute_admissions_basic_synthetic.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e3e49-7041-4560-a71e-5dd051263d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have now extracted our data and we can see it is very messy and incomplete. The so called SNOMED codes are actually ICD9 codes so we need to tidy them up\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35b4cf-5afe-4bde-865b-e48afed3fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the columns like this\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9562fd0-6543-4452-ac7c-8348e0429847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ReMapping\n",
    "\n",
    "column_remapper = {\n",
    "    'ROW_ID'     :'rowid', \n",
    "     'SUBJECT_ID' :'patientid', \n",
    "     'ETHNICITY'  :'ethnos', \n",
    "     'ADMITTIME'  :'admitdate', \n",
    "     'ADMITTIME.1':'admittime',\n",
    "       'DISCHTIME':'dischdatetime', # Not required for study but can be used to calc LOS \n",
    "          'GENDER':'gender', \n",
    "             'AGE':'activage', \n",
    "     'SNOMED_CODE':'icd9_code'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b5031-c520-414a-a605-80838e6ed4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we just apply this to the data to start tidying it up\n",
    "\n",
    "df.columns = df.columns.map(column_remapper)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f4a99-e927-4fdc-a6f9-d42672a9acb7",
   "metadata": {},
   "source": [
    "## First Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf59bd-86fb-48c2-9374-d470f747f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we are going to try and run the validator over the data. It will not go well but it allows you to start playing with the validator\n",
    "\n",
    "# First make a copy of the data\n",
    "dfa = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1c8cd-feaf-4cc2-bf79-d002ea5eed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then run the validator over the data\n",
    "\n",
    "good, bad = aa.data.validate.validate_admitted_care_data(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c794d-d95a-4a3d-be7d-35351694214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total number of rows in input data   : {dfa.shape[0]}\n",
    "Number of rows that passed validation: {good.shape[0]}\n",
    "Number of rows that failed validation: {bad.shape[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a918be6-1475-4ea8-97c7-accad096374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad[[\"schema_context\", \"column\", \"check\", \"check_number\", \"failure_case\", \"index\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de091fa-5b3d-4fd8-b7d5-289146fd0cb4",
   "metadata": {},
   "source": [
    "That didn't work and in this case we would need an entirely fresh extract as so much is missing. In the next example we will use a more complete dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "67f309c84aef927e23370ac3f0eff32084ea4ac673a1eba0bd21453e4bffc4a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
